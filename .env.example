########################################
# Hugging Face
########################################
# Personal access token with “Read” scope only.
# Get one at https://huggingface.co/settings/tokens
HF_TOKEN=hf_your_read_only_token_here


########################################
# Model selection
########################################
# Name you will register inside Ollama.
# Must match a Modelfile-<name> in ./models  (e.g. Modelfile-gemma-q4s)
OLLAMA_MODEL=gemma27b_q4s

# GGUF file to download from HF Hub (no spaces)
GGUF_FILE=mlabonne_gemma-3-27b-it-abliterated-Q4_K_S.gguf

# HF repo that hosts the GGUF
GGUF_REPO=bartowski/mlabonne_gemma-3-27b-it-abliterated-GGUF


########################################
# Runtime tuning (edit per GPU size)
########################################
# Transformer layers kept on GPU (each ~25 MB VRAM for 27 B)
NUM_GPU=60

# Maximum context window (tokens)
NUM_CTX=8192

# Sampling defaults
TEMPERATURE=0.7
TOP_K=40
TOP_P=0.9


########################################
# Networking / exposure
########################################
# Ollama will listen inside container on 11434.
# Change host port below if 11434 is occupied.
HOST_PORT=11434
